1.iloc:
dataset = pd.read_csv(Data.csv)
# X 获取所有行从第0列到倒数第1列的数据（从0起算）
X = dataset.iloc[ : ,:-1].values
# Y 获取所有行第3列的数据
Y = dataset.iloc[ : , 3].values
iloc 的作用是通过行列号来获取数据，而 loc 则是通过标签索引数据

2.Imputer:
填补缺失值：sklearn.preprocessing.Imputer(missing_values=’NaN’, strategy=’mean’, axis=0, verbose=0, copy=True)

主要参数说明：

missing_values：缺失值，可以为整数或NaN(缺失值numpy.nan用字符串‘NaN’表示)，默认为NaN

strategy：替换策略，字符串，默认用均值‘mean’替换

①若为mean时，用特征列的均值替换

②若为median时，用特征列的中位数替换

③若为most_frequent时，用特征列的众数替换

axis：指定轴数，默认axis=0代表列，axis=1代表行

copy：设置为True代表不在原数据集上修改，设置为False时，就地修改，存在如下情况时，即使设置为False时，也不会就地修改

①X不是浮点值数组

②X是稀疏且missing_values=0

③axis=0且X为CRS矩阵

④axis=1且X为CSC矩阵

3.KNN(KNeighborsClassifier)k近邻算法
K最近邻(k-Nearest Neighbor，KNN)分类算法的核心思想是如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。

4.DataFrame
DataFrame是Python中Pandas库中的一种数据结构，它类似excel，是一种二维表。
pandas.DataFrame( data, index, columns, dtype, copy)
 

data表示要传入的数据 ，包括 ndarray，series，map，lists，dict，constant和另一个DataFrame

index和columns 行索引和列索引  格式['x1','x2']

dtype:每列的类型

copy: 查了api，才知道意思是从input输入中拷贝数据。默认是false，不拷贝

5.\r   \n   \r\n
\r:回到本行行首
\n:换行
\r\n：Enter

6.pd.read_csv()参数
filepath_or_buffer：读取的对象，是一个字符串，该字符串可以是URL，包括http，ftp，本地文件
sep：分隔符，默认是‘,’，CSV文件的分隔符
delimiter：sep的替代参数，默认为None
header：列名，int或int列表，默认是infer，即默认第一行为列名
names：要使用的列名(columns)，array-like，默认是None，当读取的CSV文件没有列名时，可以通过names参数指定
index_col：要是用的行名(index)，int或sequence或False，默认为None，即默认添加从0开始的index

7.pandas的reset_index()
This is useful when the index needs to be treated as a column, or when the index is meaningless and needs to be reset to the default before another operation.
当索引需要作为列处理时，或者当索引没有意义并且需要在另一个操作之前重置为默认值时，这是非常有用的。

8.scipy.misc作用
SciPy 中包含一些用于输入和输出的实用模块。下面介绍其中两个模块：io 和misc。
以图像形式保存数组
因为我们需要对图像进行操作，并且需要使用数组对象来做运算，所以将数组直接保存为图像文件1 非常有用。

9.from scipy.misc import imread
imread()以数组的形式读取图片

10.image_colors = ImageColorGenerator(alice_coloring)
以背景图片作为颜色填充

11.plt.axis("off")
off:表示关闭轴线和标签

12.jieba.posseg.cut(text)
jieba.posseg.cut(sen):返回的每个迭代对象有两个属性-> word 词语 + flag 词性

example:
import jieba.posseg
words = jieba.posseg.cut(sen)
for word in words:
    print(word.flag," ",word.word)
    
  
   
result:
ns   胶州市
n   市长
x   江大桥

13.Python中的TfidfVectorizer参数解析
vectorizer = CountVectorizer() #构建一个计算词频（TF）的玩意儿，当然这里面不足是可以做这些

transformer = TfidfTransformer() #构建一个计算TF-IDF的玩意儿

tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))

#vectorizer.fit_transform(corpus)将文本corpus输入，得到词频矩阵

#将这个矩阵作为输入，用transformer.fit_transform(词频矩阵)得到TF-IDF权重矩阵


TfidfTransformer + CountVectorizer  =  TfidfVectorizer


值得注意的是，CountVectorizer()和TfidfVectorizer()里面都有一个成员叫做vocabulary_（后面带一个下划线）

这个成员的意义是词典索引，对应的是TF-IDF权重矩阵的列，只不过一个是私有成员，一个是外部输入，原则上应该保持一致。


vectorizer = TfidfVectorizer(stop_words=stpwrdlst, sublinear_tf = True, max_df = 0.5)


关于参数：

input：string{'filename', 'file', 'content'}

    如果是'filename'，序列作为参数传递给拟合器，预计为文件名列表，这需要读取原始内容进行分析

    如果是'file'，序列项目必须有一个”read“的方法（类似文件的对象），被调用作为获取内存中的字节数

    否则，输入预计为序列串，或字节数据项都预计可直接进行分析。

encoding：string， ‘utf-8’by default

    如果给出要解析的字节或文件，此编码将用于解码

decode_error: {'strict', 'ignore', 'replace'}

    如果一个给出的字节序列包含的字符不是给定的编码，指示应该如何去做。默认情况下，它是'strict'，这意味着的UnicodeDecodeError将提高，其他值是'ignore'和'replace'

strip_accents: {'ascii', 'unicode', None}

    在预处理步骤中去除编码规则(accents)，”ASCII码“是一种快速的方法，仅适用于有一个直接的ASCII字符映射，"unicode"是一个稍慢一些的方法，None（默认）什么都不做

analyzer：string，{'word', 'char'} or callable

    定义特征为词（word）或n-gram字符，如果传递给它的调用被用于抽取未处理输入源文件的特征序列

preprocessor：callable or None（default）

    当保留令牌和”n-gram“生成步骤时，覆盖预处理（字符串变换）的阶段

tokenizer：callable or None(default)

    当保留预处理和n-gram生成步骤时，覆盖字符串令牌步骤

ngram_range: tuple(min_n, max_n)

    要提取的n-gram的n-values的下限和上限范围，在min_n <= n <= max_n区间的n的全部值

stop_words：string {'english'}, list, or None(default)

    如果未english，用于英语内建的停用词列表

    如果未list，该列表被假定为包含停用词，列表中的所有词都将从令牌中删除

    如果None，不使用停用词。max_df可以被设置为范围[0.7, 1.0)的值，基于内部预料词频来自动检测和过滤停用词

lowercase：boolean， default True

    在令牌标记前转换所有的字符为小写

token_pattern：string

    正则表达式显示了”token“的构成，仅当analyzer == ‘word’时才被使用。两个或多个字母数字字符的正则表达式（标点符号完全被忽略，始终被视为一个标记分隔符）。

max_df： float in range [0.0, 1.0] or int, optional, 1.0 by default

    当构建词汇表时，严格忽略高于给出阈值的文档频率的词条，语料指定的停用词。如果是浮点值，该参数代表文档的比例，整型绝对计数值，如果词汇表不为None，此参数被忽略。

min_df：float in range [0.0, 1.0] or int, optional, 1.0 by default

当构建词汇表时，严格忽略低于给出阈值的文档频率的词条，语料指定的停用词。如果是浮点值，该参数代表文档的比例，整型绝对计数值，如果词汇表不为None，此参数被忽略。

max_features： optional， None by default

    如果不为None，构建一个词汇表，仅考虑max_features--按语料词频排序，如果词汇表不为None，这个参数被忽略

vocabulary：Mapping or iterable， optional

    也是一个映射（Map）（例如，字典），其中键是词条而值是在特征矩阵中索引，或词条中的迭代器。如果没有给出，词汇表被确定来自输入文件。在映射中索引不能有重复，并且不能在0到最大索引值之间有间断。

binary：boolean， False by default

    如果未True，所有非零计数被设置为1，这对于离散概率模型是有用的，建立二元事件模型，而不是整型计数

dtype：type， optional

    通过fit_transform()或transform()返回矩阵的类型

norm：'l1', 'l2', or None,optional

    范数用于标准化词条向量。None为不归一化

use_idf：boolean， optional

    启动inverse-document-frequency重新计算权重

smooth_idf：boolean，optional

    通过加1到文档频率平滑idf权重，为防止除零，加入一个额外的文档

sublinear_tf：boolean， optional

    应用线性缩放TF，例如，使用1+log(tf)覆盖tf
    
    
 14.sklearn中的cross_val_score()函数
sklearn.cross_validation.cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch=‘2*n_jobs’)

参数
estimator：数据对象 
X：数据 
y：预测数据 
soring：调用的方法
cv：交叉验证生成器或可迭代的次数 
n_jobs：同时工作的cpu个数（-1代表全部）
verbose：详细程度
fit_params：传递给估计器的拟合方法的参数
pre_dispatch：控制并行执行期间调度的作业数量。减少这个数量对于避免在CPU发送更多作业时CPU内存消耗的扩大是有用的。该参数可以是：

没有，在这种情况下，所有的工作立即创建并产生。将其用于轻量级和快速运行的作业，以避免由于按需产生作业而导致延迟
一个int，给出所产生的总工作的确切数量
一个字符串，给出一个表达式作为n_jobs的函数，如'2 * n_jobs'

15.os.path.join()
用os.path.join()连接两个文件名地址的时候，就比os.path.join("D:\","test.txt")结果是D:\test.txt，并且在我们往里面写东西，然后保存，在这个目录下会生成这个文件，但是如果你不写东西，那么执行这句话之后，在D盘的目录下是不会有这个文件名称的

16.torch.save()
1. 先建立一个字典，保存三个参数：

state = {‘net':model.state_dict(), 'optimizer':optimizer.state_dict(), 'epoch':epoch}

2.调用torch.save():

torch.save(state, dir)
state = {‘net':model.state_dict(), 'optimizer':optimizer.state_dict(), 'epoch':epoch}
其中dir表示保存文件的绝对路径+保存文件名，如'/home/qinying/Desktop/modelpara.pth'


16.format()
print('she is  {}'.format('lily'))
result: she is  lily

17.items()
Python 字典(Dictionary) items() 函数以列表返回可遍历的(键, 值) 元组数组。
example:
dict = {'Google': 'www.google.com', 'Runoob': 'www.runoob.com', 'taobao': 'www.taobao.com'}
print(dict.items())
for key,value in dict.items():
    print(key+'----------'+ value)
    
result:
dict_items([('Google', 'www.google.com'), ('Runoob', 'www.runoob.com'), ('taobao', 'www.taobao.com')])
Google----------www.google.com
Runoob----------www.runoob.com
taobao----------www.taobao.com

18.add_argument（）
参数解析：https://wenku.baidu.com/view/48cce7e8bcd126fff6050b08.html

19.parser.parse_args()
我对与文档的理解是，parse_args()是将之前add_argument()定义的参数进行赋值，并返回相关的namespace

20.__dict__
　首先看一下类的__dict__属性和类对象的__dict__属性
  类的静态函数、类函数、普通函数、全局变量以及一些内置的属性都是放在类__dict__里的
  对象的__dict__中存储了一些self.xxx的一些东西
  https://www.cnblogs.com/alvin2010/p/9102344.html
  
 
 21.nn.Embedding()
 word embedding的定义nn.Embedding(2, 5)，这里的2表示有2个词，5表示5维度，其实也就是一个2x5的矩阵，所以如果你有1000个词，每个词希望是100维，你就可以这样建立一个word embedding，nn.Embedding(1000, 100)。
 
 22.nn.Linear()
 class torch.nn.Linear(in_features, out_features, bias=True)
对输入数据做线性变换：y=Ax+b

参数：

in_features - 每个输入样本的大小
out_features - 每个输出样本的大小
bias - 若设置为False，这层不会学习偏置。默认值：True

23.nn.LSTM()
参数说明:

input_size – 输入的特征维度
hidden_size – 隐状态的特征维度
num_layers – 层数（和时序展开要区分开）
bias – 如果为False，那么LSTM将不会使用$b_{ih},b_{hh}$，默认为True。
batch_first – 如果为True，那么输入和输出Tensor的形状为(batch, seq, feature)
dropout – 如果非零的话，将会在RNN的输出上加个dropout，最后一层除外。
bidirectional – 如果为True，将会变成一个双向RNN，默认为False。
 
24.squeeze()
squeeze(input，axis) 这里input 是个张量,如果axis维度是一，将此维度压缩。
张量.squeeze(0) 0表示第一维度，1表示第二维度。  若果值为1，压缩。否则不压缩
https://blog.csdn.net/lanse_zhicheng/article/details/79148678


25.Unicode
Unicode（统一码、万国码、单一码）是计算机科学领域里的一项业界标准，包括字符集、编码方案等。Unicode 是为了解决传统的字符编码方案的局限而产生的，
它为每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台进行文本转换、处理的要求。1990年开始研发，1994年正式公布.

26.unicodedata.category(unichr)
返回一个字符串, 描述unichr的一般类别。

27.unicodedata.normalize()
normalize() 第一个参数指定字符串标准化的方式。 NFC表示字符应该是整体组成(比如可能的话就使用单一编码)，而NFD表示字符应该分解为多个组合字符表示
 t1 = unicodedata.normalize('NFC', s1)
 以 NFC的方式标准化s1
 
28.python基础
https://docs.python.org/3.6/tutorial/datastructures.html#dictionaries



