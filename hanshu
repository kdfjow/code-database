1.iloc:
dataset = pd.read_csv(Data.csv)
# X 获取所有行从第0列到倒数第1列的数据（从0起算）
X = dataset.iloc[ : ,:-1].values
# Y 获取所有行第3列的数据
Y = dataset.iloc[ : , 3].values
iloc 的作用是通过行列号来获取数据，而 loc 则是通过标签索引数据

2.Imputer:
填补缺失值：sklearn.preprocessing.Imputer(missing_values=’NaN’, strategy=’mean’, axis=0, verbose=0, copy=True)

主要参数说明：

missing_values：缺失值，可以为整数或NaN(缺失值numpy.nan用字符串‘NaN’表示)，默认为NaN

strategy：替换策略，字符串，默认用均值‘mean’替换

①若为mean时，用特征列的均值替换

②若为median时，用特征列的中位数替换

③若为most_frequent时，用特征列的众数替换

axis：指定轴数，默认axis=0代表列，axis=1代表行

copy：设置为True代表不在原数据集上修改，设置为False时，就地修改，存在如下情况时，即使设置为False时，也不会就地修改

①X不是浮点值数组

②X是稀疏且missing_values=0

③axis=0且X为CRS矩阵

④axis=1且X为CSC矩阵

3.KNN(KNeighborsClassifier)k近邻算法
K最近邻(k-Nearest Neighbor，KNN)分类算法的核心思想是如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。

4.DataFrame
DataFrame是Python中Pandas库中的一种数据结构，它类似excel，是一种二维表。
pandas.DataFrame( data, index, columns, dtype, copy)
 

data表示要传入的数据 ，包括 ndarray，series，map，lists，dict，constant和另一个DataFrame

index和columns 行索引和列索引  格式['x1','x2']

dtype:每列的类型

copy: 查了api，才知道意思是从input输入中拷贝数据。默认是false，不拷贝

5.\r   \n   \r\n
\r:回到本行行首
\n:换行
\r\n：Enter

6.pd.read_csv()参数
filepath_or_buffer：读取的对象，是一个字符串，该字符串可以是URL，包括http，ftp，本地文件
sep：分隔符，默认是‘,’，CSV文件的分隔符
delimiter：sep的替代参数，默认为None
header：列名，int或int列表，默认是infer，即默认第一行为列名
names：要使用的列名(columns)，array-like，默认是None，当读取的CSV文件没有列名时，可以通过names参数指定
index_col：要是用的行名(index)，int或sequence或False，默认为None，即默认添加从0开始的index

7.pandas的reset_index()
This is useful when the index needs to be treated as a column, or when the index is meaningless and needs to be reset to the default before another operation.
当索引需要作为列处理时，或者当索引没有意义并且需要在另一个操作之前重置为默认值时，这是非常有用的。

8.scipy.misc作用
SciPy 中包含一些用于输入和输出的实用模块。下面介绍其中两个模块：io 和misc。
以图像形式保存数组
因为我们需要对图像进行操作，并且需要使用数组对象来做运算，所以将数组直接保存为图像文件1 非常有用。

9.from scipy.misc import imread
imread()以数组的形式读取图片

10.image_colors = ImageColorGenerator(alice_coloring)
以背景图片作为颜色填充

11.plt.axis("off")
off:表示关闭轴线和标签

12.jieba.posseg.cut(text)
jieba.posseg.cut(sen):返回的每个迭代对象有两个属性-> word 词语 + flag 词性

example:
import jieba.posseg
words = jieba.posseg.cut(sen)
for word in words:
    print(word.flag," ",word.word)
    
  
   
result:
ns   胶州市
n   市长
x   江大桥

13.Python中的TfidfVectorizer参数解析
vectorizer = CountVectorizer() #构建一个计算词频（TF）的玩意儿，当然这里面不足是可以做这些

transformer = TfidfTransformer() #构建一个计算TF-IDF的玩意儿

tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))

#vectorizer.fit_transform(corpus)将文本corpus输入，得到词频矩阵

#将这个矩阵作为输入，用transformer.fit_transform(词频矩阵)得到TF-IDF权重矩阵


TfidfTransformer + CountVectorizer  =  TfidfVectorizer


值得注意的是，CountVectorizer()和TfidfVectorizer()里面都有一个成员叫做vocabulary_（后面带一个下划线）

这个成员的意义是词典索引，对应的是TF-IDF权重矩阵的列，只不过一个是私有成员，一个是外部输入，原则上应该保持一致。


vectorizer = TfidfVectorizer(stop_words=stpwrdlst, sublinear_tf = True, max_df = 0.5)


关于参数：

input：string{'filename', 'file', 'content'}

    如果是'filename'，序列作为参数传递给拟合器，预计为文件名列表，这需要读取原始内容进行分析

    如果是'file'，序列项目必须有一个”read“的方法（类似文件的对象），被调用作为获取内存中的字节数

    否则，输入预计为序列串，或字节数据项都预计可直接进行分析。

encoding：string， ‘utf-8’by default

    如果给出要解析的字节或文件，此编码将用于解码

decode_error: {'strict', 'ignore', 'replace'}

    如果一个给出的字节序列包含的字符不是给定的编码，指示应该如何去做。默认情况下，它是'strict'，这意味着的UnicodeDecodeError将提高，其他值是'ignore'和'replace'

strip_accents: {'ascii', 'unicode', None}

    在预处理步骤中去除编码规则(accents)，”ASCII码“是一种快速的方法，仅适用于有一个直接的ASCII字符映射，"unicode"是一个稍慢一些的方法，None（默认）什么都不做

analyzer：string，{'word', 'char'} or callable

    定义特征为词（word）或n-gram字符，如果传递给它的调用被用于抽取未处理输入源文件的特征序列

preprocessor：callable or None（default）

    当保留令牌和”n-gram“生成步骤时，覆盖预处理（字符串变换）的阶段

tokenizer：callable or None(default)

    当保留预处理和n-gram生成步骤时，覆盖字符串令牌步骤

ngram_range: tuple(min_n, max_n)

    要提取的n-gram的n-values的下限和上限范围，在min_n <= n <= max_n区间的n的全部值

stop_words：string {'english'}, list, or None(default)

    如果未english，用于英语内建的停用词列表

    如果未list，该列表被假定为包含停用词，列表中的所有词都将从令牌中删除

    如果None，不使用停用词。max_df可以被设置为范围[0.7, 1.0)的值，基于内部预料词频来自动检测和过滤停用词

lowercase：boolean， default True

    在令牌标记前转换所有的字符为小写

token_pattern：string

    正则表达式显示了”token“的构成，仅当analyzer == ‘word’时才被使用。两个或多个字母数字字符的正则表达式（标点符号完全被忽略，始终被视为一个标记分隔符）。

max_df： float in range [0.0, 1.0] or int, optional, 1.0 by default

    当构建词汇表时，严格忽略高于给出阈值的文档频率的词条，语料指定的停用词。如果是浮点值，该参数代表文档的比例，整型绝对计数值，如果词汇表不为None，此参数被忽略。

min_df：float in range [0.0, 1.0] or int, optional, 1.0 by default

当构建词汇表时，严格忽略低于给出阈值的文档频率的词条，语料指定的停用词。如果是浮点值，该参数代表文档的比例，整型绝对计数值，如果词汇表不为None，此参数被忽略。

max_features： optional， None by default

    如果不为None，构建一个词汇表，仅考虑max_features--按语料词频排序，如果词汇表不为None，这个参数被忽略

vocabulary：Mapping or iterable， optional

    也是一个映射（Map）（例如，字典），其中键是词条而值是在特征矩阵中索引，或词条中的迭代器。如果没有给出，词汇表被确定来自输入文件。在映射中索引不能有重复，并且不能在0到最大索引值之间有间断。

binary：boolean， False by default

    如果未True，所有非零计数被设置为1，这对于离散概率模型是有用的，建立二元事件模型，而不是整型计数

dtype：type， optional

    通过fit_transform()或transform()返回矩阵的类型

norm：'l1', 'l2', or None,optional

    范数用于标准化词条向量。None为不归一化

use_idf：boolean， optional

    启动inverse-document-frequency重新计算权重

smooth_idf：boolean，optional

    通过加1到文档频率平滑idf权重，为防止除零，加入一个额外的文档

sublinear_tf：boolean， optional

    应用线性缩放TF，例如，使用1+log(tf)覆盖tf
    
    
 14.sklearn中的cross_val_score()函数
sklearn.cross_validation.cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch=‘2*n_jobs’)

参数
estimator：数据对象 
X：数据 
y：预测数据 
soring：调用的方法
cv：交叉验证生成器或可迭代的次数 
n_jobs：同时工作的cpu个数（-1代表全部）
verbose：详细程度
fit_params：传递给估计器的拟合方法的参数
pre_dispatch：控制并行执行期间调度的作业数量。减少这个数量对于避免在CPU发送更多作业时CPU内存消耗的扩大是有用的。该参数可以是：

没有，在这种情况下，所有的工作立即创建并产生。将其用于轻量级和快速运行的作业，以避免由于按需产生作业而导致延迟
一个int，给出所产生的总工作的确切数量
一个字符串，给出一个表达式作为n_jobs的函数，如'2 * n_jobs'
